{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Main Figures / Tables\n",
    "\n",
    "**Note**: See `README.md` in the main folder if you have not already, for instructions on how to generate the experiment results that this notebook uses.\n",
    "\n",
    "This notebook will replicate tables in the main paper, and generate data for plotting figures, which is done via separate `R` scripts. \n",
    "\n",
    "After running this notebook end-to-end, run `plot_all.sh` which will run all of the relevant `R` scripts (`plot_figure_[2-5].R` and `plot_figure_s[2-3].R`) and populate `./figures/` with the relevant figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This flag can be set to use the same hyperparameters and thresholds \n",
    "# as in our published work.  Due to differences in features (described in README.md), there are still\n",
    "# some minor differences, but this will more closely replicate our published results\n",
    "USE_REP_HP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = os.environ['DATA_PATH']\n",
    "REPO_HOME = os.environ['REPO_PATH']\n",
    "sys.path.insert(0, REPO_HOME)\n",
    "\n",
    "EXP_PATH = os.environ['EXP_RESULT_PATH']\n",
    "THRESH_TRAIN_PATH = os.environ['THRESHOLD_RESULT_PATH']\n",
    "TRAIN_MODEL_PATH = os.environ['VAL_OUTCOME_MODEL_PATH']\n",
    "\n",
    "if USE_REP_HP:\n",
    "    PRED_PATH = os.environ['TEST_OUTCOME_MODEL_PATH_REP']\n",
    "else:\n",
    "    PRED_PATH = os.environ['TEST_OUTCOME_MODEL_PATH']\n",
    "\n",
    "THRESH_TEST_PATH = f\"{EXP_PATH}/thresholding/thresholding_eval_test/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = f\"{REPO_HOME}/notebooks/fig_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(OUT_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_utils import model_analysis_utils as ma_utils\n",
    "from analysis_utils import policy_analysis_utils as policy_utils\n",
    "from analysis_utils import best_case_baseline_utils as bca_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data\n",
    "test_features = pd.read_csv(f\"{DATA_PATH}/test_uncomp_uti_features.csv\")\n",
    "train_features = pd.read_csv(f\"{DATA_PATH}/train_uncomp_uti_features.csv\")\n",
    "\n",
    "test_labels = pd.read_csv(f\"{DATA_PATH}/test_uncomp_resist_data.csv\")\n",
    "train_labels = pd.read_csv(f\"{DATA_PATH}/train_uncomp_resist_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test set predictions \n",
    "all_preds = pd.read_csv(f\"{PRED_PATH}/test_predictions.csv\")#.query('is_train == 0').drop(['is_train'], axis=1)\n",
    "test_preds_actual = pd.merge(all_preds, test_labels, on='example_id')\n",
    "train_preds_actual = pd.merge(all_preds, train_labels, on='example_id') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all samples with previous exposure/resistance\n",
    "test_prior_hist_eids = policy_utils.get_prior_exposure_examples(test_features)\n",
    "test_prior_hist_preds_actual = test_preds_actual.query('example_id in @test_prior_hist_eids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we obtain the `policy_df`, which specifies the recommendations made by the algorithm.  This can either be loaded directly from the saved results of `thresholding_eval_test.sh`, or it can be manually re-constructed.  \n",
    "\n",
    "We will do the latter here, in order to demonstrate both (a) the results obtained by running our code end-to-end, and (b) the results obtained by plugging in the thresholds chosen in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.indirect.policy_learning_thresholding as plearn\n",
    "\n",
    "abx_list=['NIT', 'SXT', 'CIP', 'LVX']\n",
    "\n",
    "if not USE_REP_HP:\n",
    "    val_outcomes_by_setting = pd.read_csv(f\"{THRESH_TRAIN_PATH}/val_stats_by_setting.csv\")\n",
    "\n",
    "    constraint = 0.1\n",
    "\n",
    "    best_setting = val_outcomes_by_setting[\n",
    "        val_outcomes_by_setting['broad_prop'] <= constraint\n",
    "    ].sort_values(by='iat_prop').iloc[0]\n",
    "\n",
    "    curr_setting = dict(zip(abx_list, [{'vme': best_setting[abx]} for abx in abx_list]))\n",
    "\n",
    "    # Note that we choose the VME (i.e., false susceptibility rate) using the validation splits\n",
    "    # and then the \"optimal threshold\" corresponding to this VME is re-computed across the entire\n",
    "    # training set\n",
    "    thresholds = plearn.get_thresholds_dict(train_preds_actual, curr_setting, abx_list=abx_list)\n",
    "\n",
    "    print(curr_setting)\n",
    "    print(thresholds)\n",
    "else:\n",
    "    # Due to the nuances of train/validate splits being different in replication,\n",
    "    # the above thresholds are different than those used in the published results.\n",
    "    # The published thresholds are given below, and can be used to more closely replicate our published figures.\n",
    "\n",
    "    # However, note that the results will still not be exactly the same, in part due to slight differences in the features, \n",
    "    # (as noted above) and therefore the trained models\n",
    "    thresholds = {\n",
    "        'NIT': 0.129,\n",
    "        'SXT': 0.180,\n",
    "        'CIP': 0.258,\n",
    "        'LVX': 0.239,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_df = plearn.get_policy_for_preds(test_preds_actual, thresholds,\n",
    "                                             abx_list=abx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_REP_HP:\n",
    "    # We can verify that this policy_df matches the one saved on disk (assuming that we are NOT using the published thresholds)\n",
    "    policy_df_saved = pd.read_csv(f\"{THRESH_TEST_PATH}/test_policy_df.csv\")\n",
    "    assert np.all(np.equal(policy_df['rec_final'], policy_df_saved['rec_final']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 (Schematic of analytic protocol) is not replicated here, as it is a conceptual figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2: Threshold sensitivity analysis\n",
    "\n",
    "Note: This figure is only generated if you are using the end-to-end analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_REP_HP:\n",
    "    val_outcomes_by_setting.to_csv(f\"{OUT_DIR}/figure_2_threshold_sensitivity.csv\")\n",
    "\n",
    "    # These values are plugged into the corresponding R script\n",
    "    print(f\"IAT: {best_setting['iat_prop']}\\nSpectrum: {best_setting['broad_prop']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3: False susceptibility and non-susceptibility rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_fnr_df = ma_utils.create_fpr_fnr_data(test_preds_actual)\n",
    "fpr_fnr_df.to_csv(f\"{OUT_DIR}/figure_3_fpr_fnr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_plot = {}\n",
    "for name in [('Nitrofurantoin', 'NIT'), \n",
    "             ('TMP-SMX', 'SXT'),\n",
    "             ('Ciprofloxacin', 'CIP'),\n",
    "             ('Levofloxacin', 'LVX')]:\n",
    "    thresholds_plot[name[0]] = thresholds[name[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_df = pd.DataFrame.from_dict(thresholds_plot, orient='index').reset_index()\n",
    "thresh_df = thresh_df.rename(columns={'index': 'drug', 0: 'value'})\n",
    "thresh_df.to_csv(f\"{OUT_DIR}/figure_3_chosen_thresh.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4: Post hoc analysis of clinician vs algorithm therapy decisions and appropriateness in patients with uncomplicated UTI presenting between 2014 and 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakdown_df = policy_utils.get_doc_alg_breakdown(policy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_df = pd.DataFrame(np.zeros((16, 5)))\n",
    "template_df = template_df.rename(columns = {\n",
    "    0: \"Decision-maker\",\n",
    "    1: \"Drug\",\n",
    "    2: \"Result\",\n",
    "    3: \"Comparator\",\n",
    "    4: \"Value\"\n",
    "})\n",
    "\n",
    "i = 0\n",
    "for compare in ['MD_narrow_IAT', 'MD_narrow_noIAT', 'MD_broad_IAT', 'MD_broad_noIAT']:\n",
    "    for line in ['First line', 'Second line']:\n",
    "        for res in ['Inappropriate', 'Appropriate']:\n",
    "            template_df.iloc[i] = ('Model', line, res, compare, -1)\n",
    "            i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis_df = policy_utils.format_breakdown_df(template_df, breakdown_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_analysis_df.to_csv(f\"{OUT_DIR}/figure_4_error_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5: Feature importance characterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This takes a few minutes to run, because it performs the relevant analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if USE_REP_HP:\n",
    "    with open(f\"{REPO_HOME}/models/replication_hyperparameters/best_models.json\") as f:\n",
    "        best_model_class = json.load(f)\n",
    "\n",
    "    with open(f\"{REPO_HOME}/models/replication_hyperparameters/hyperparameters.json\") as f:\n",
    "        best_params = json.load(f)\n",
    "else:\n",
    "    with open(f\"{TRAIN_MODEL_PATH}/best_models.json\") as f:\n",
    "        best_model_class = json.load(f)\n",
    "\n",
    "    with open(f\"{TRAIN_MODEL_PATH}/hyperparameters.json\") as f:\n",
    "        best_params = json.load(f)\n",
    "\n",
    "figure_5_df = policy_utils.run_feature_importance_analysis(\n",
    "    train_features, train_labels,\n",
    "    test_features, test_labels,\n",
    "    model_class_dict = best_model_class, \n",
    "    best_params_dict = best_params)\n",
    "\n",
    "figure_5_df.to_csv(f\"{OUT_DIR}/figure_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1 is not replicated here, as the statistics are computed on a per-patient basis, as opposed to a per-sample basis, and patient identifiers are not included in this dataset release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2: AUROCs for prediction of antibiotic non-susceptibility in patients presenting with uncomplicated UTI between 2014 and 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.indirect.train_outcome_models import evaluate_test_cohort\n",
    "\n",
    "import json\n",
    "\n",
    "if USE_REP_HP:\n",
    "    with open(f\"{REPO_HOME}/models/replication_hyperparameters/best_models.json\") as f:\n",
    "        best_model_class = json.load(f)\n",
    "\n",
    "    with open(f\"{REPO_HOME}/models/replication_hyperparameters/hyperparameters.json\") as f:\n",
    "        best_params = json.load(f)\n",
    "else:\n",
    "    with open(f\"{TRAIN_MODEL_PATH}/best_models.json\") as f:\n",
    "        best_model_class = json.load(f)\n",
    "\n",
    "    with open(f\"{TRAIN_MODEL_PATH}/hyperparameters.json\") as f:\n",
    "        best_params = json.load(f)\n",
    "        \n",
    "abx_name_map = {\n",
    "    'NIT': 'Nitrofurantoin',\n",
    "    'SXT': 'TMP-SMX',\n",
    "    'CIP': 'Ciprofloxacin',\n",
    "    'LVX': 'Levofloxacin',\n",
    "}\n",
    "\n",
    "test_prior_hist_eids = policy_utils.get_prior_exposure_examples(test_features)\n",
    "train_prior_hist_eids = policy_utils.get_prior_exposure_examples(train_features)\n",
    "\n",
    "prior_hist_eids = np.concatenate([train_prior_hist_eids, test_prior_hist_eids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2 = []\n",
    "auc_dict = {}\n",
    "\n",
    "auc_dict['full'], _ , _ = evaluate_test_cohort(train_features, train_labels,\n",
    "                                                      test_features, test_labels, \n",
    "                                                      best_params, best_model_class,\n",
    "                                                      subcohort_eids=None)\n",
    "\n",
    "auc_dict['prior'], _ , _ = evaluate_test_cohort(train_features, train_labels,\n",
    "                                                      test_features.query('example_id in @test_prior_hist_eids'), \n",
    "                                                      test_labels.query('example_id in @test_prior_hist_eids'), \n",
    "                                                      best_params, best_model_class,\n",
    "                                                      subcohort_eids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort, auc_result in auc_dict.items():\n",
    "    for drug_code, drug_name in abx_name_map.items():\n",
    "        mean_auc, stdev_auc, ci_auc = auc_result[drug_code]\n",
    "\n",
    "        table_2.append([cohort, drug_name,\n",
    "                      mean_auc.round(2), ci_auc[0].round(2), ci_auc[1].round(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 3: Comparison of primary outcomes for algorithm, clinicians and best-case guideline-based policy in patients presenting with uncomplicated UTI between 2014 and 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_df = policy_df.rename(\n",
    "        columns={'rec': 'alg_raw', 'rec_final': 'alg', 'prescription': 'doc'}\n",
    "    )\n",
    "\n",
    "prev_resist_NIT = list(test_features[test_features['micro - prev resistance NIT 90'] == 1].example_id.values)\n",
    "prev_exposure_NIT = list(test_features[test_features['medication 90 - nitrofurantoin'] == 1].example_id.values)\n",
    "avoid_NIT_eids = set(prev_resist_NIT).union(set(prev_exposure_NIT))\n",
    "\n",
    "policy_df['idsa'] = policy_df.apply(lambda x: 'CIP' if x['example_id'] in set(avoid_NIT_eids) else 'NIT',\n",
    "                                   axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_3a = policy_utils.compile_all_stats(policy_df)\n",
    "\n",
    "# Get the decisions for the modified guideline\n",
    "policy_df_decision = policy_df[policy_df['alg_raw'] != 'defer']\n",
    "table_3b_all = bca_utils.get_best_case_stats(policy_df, p=.18)\n",
    "table_3b_decision = bca_utils.get_best_case_stats(policy_df_decision, p=.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dict = {\n",
    "    'broad': {\n",
    "        'decision': {},\n",
    "        'all': {}\n",
    "    },\n",
    "    'iat': {\n",
    "        'decision': {},\n",
    "        'all': {}\n",
    "    },\n",
    "}\n",
    "\n",
    "best_guideline = {\n",
    "    'decision': {\n",
    "        'iat': table_3b_decision[1]['all'],\n",
    "        'broad': table_3b_decision[0]['all']\n",
    "    }, \n",
    "    'all': {\n",
    "        'iat': table_3b_all[1]['all'],\n",
    "        'broad': table_3b_all[0]['all']        \n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "for policy in ['alg', 'doc', 'best_case_guideline']:\n",
    "    for cohort in ['decision', 'all']:\n",
    "        if policy == 'best_case_guideline':\n",
    "            table_dict['iat'][cohort][policy] = best_guideline[cohort]['iat']\n",
    "            table_dict['broad'][cohort][policy] = best_guideline[cohort]['broad']            \n",
    "        else:\n",
    "            table_dict['iat'][cohort][policy] = table_3a.query(\n",
    "                \"policy == @policy & subcohort == @cohort & antibiotic == 'all'\")[\n",
    "                ['mean_iat', 'ci_iat']].round(3).values[0].tolist()\n",
    "            table_dict['broad'][cohort][policy] = table_3a.query(\n",
    "                \"policy == @policy & subcohort == @cohort & antibiotic == 'second'\")[\n",
    "                ['mean_abx', 'ci_abx']].round(3).values[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_cohort_size = table_3a.query('policy == \"alg\" & subcohort == \"decision\" & antibiotic == \"all\"')['n'].values[0]\n",
    "full_cohort_size = table_3a.query('policy == \"alg\" & subcohort == \"all\" & antibiotic == \"all\"')['n'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Table 3\")\n",
    "for stat in ['broad', 'iat']:\n",
    "    print(f\"\\t {stat}\")\n",
    "    for cohort in ['decision', 'all']:\n",
    "        print(f\"\\t\\t {cohort}: {decision_cohort_size if cohort == 'decision' else full_cohort_size}\")\n",
    "        for policy in ['alg', 'doc', 'best_case_guideline']:\n",
    "            print(f\"\\t\\t\\t {policy}: {table_dict[stat][cohort][policy]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Selected) Supplementary Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Figures\n",
    "\n",
    "Figure S1 is not replicated here, as we do not provide time information in the dataset release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure S2: Test Set ROCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_s2_df = ma_utils.create_roc_curve_data(test_preds_actual, test_prior_hist_preds_actual)\n",
    "figure_s2_df.to_csv(f\"{OUT_DIR}/figure_s2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure S3: Calibration plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_s3_df = ma_utils.create_calibration_data_df(test_preds_actual)\n",
    "figure_s3_df.to_csv(f\"{OUT_DIR}/figure_s3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures S4-S5\n",
    "\n",
    "Figure S4 (Cohort selection) is not replicated here, as it requires data (e.g., patient identifiers) that is not present in the data release. \n",
    "\n",
    "Figure S5 (Detailed methods schematic) is not replicated here, as it is just a conceptual schematic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table S1: Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_REP_HP:\n",
    "    with open(f\"{REPO_HOME}/models/replication_hyperparameters/hyperparameters.json\") as f:\n",
    "        best_params = json.load(f)\n",
    "else:\n",
    "    with open(f\"{TRAIN_MODEL_PATH}/hyperparameters.json\") as f:\n",
    "        best_params = json.load(f)\n",
    "        \n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table S2: Detailed breakdown of primary outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This constructs the conservative IDSA guidelines\n",
    "prev_resist_NIT = list(test_features[test_features['micro - prev resistance NIT 90'] == 1].example_id.values)\n",
    "prev_exposure_NIT = list(test_features[test_features['medication 90 - nitrofurantoin'] == 1].example_id.values)\n",
    "avoid_NIT_eids = set(prev_resist_NIT).union(set(prev_exposure_NIT))\n",
    "\n",
    "policy_df['idsa'] = policy_df.apply(lambda x: 'CIP' if x['example_id'] in set(avoid_NIT_eids) else 'NIT',\n",
    "                                   axis=1)\n",
    "\n",
    "# This will give all relevant stats for a variety of antibiotic combinations, for\n",
    "# * Clinicians\n",
    "# * Algorithm\n",
    "# * Conservative IDSA guidelines\n",
    "table_s2 = policy_utils.compile_all_stats(policy_df)\n",
    "\n",
    "# To get decisions for the modified IDSA guideline (the \"best-case guidelines\"), you will need to use this function\n",
    "# bca_utils.get_best_case_stats(policy_df, p=.18)\n",
    "\n",
    "# To filter the above to a particular cohort, you can pass in a different cohort, e.g.,\n",
    "# policy_df_decision = policy_df[policy_df['alg_raw'] != 'defer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table S3: Features predicting use of fluoroquinolones\n",
    "\n",
    "This table is not replicated here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table S4: Top 10 features for prediction of non-susceptibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_coef_dict = {}\n",
    "for abx in ['NIT', 'SXT', 'CIP', 'LVX']:\n",
    "    top_coef_dict[abx] = pd.read_csv(f\"{PRED_PATH}/pos_coeffs_{abx}.csv\").round(2).values\n",
    "    print(top_coef_dict[abx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
